# TEST PATH
# test will be created in declared folder
output.base.path = results

# DATA SOURCE
input.file.path = /path/to/profile.dat
input.file.path.training = /path/to/profile_last_month.dat

# NODE TYPES NUMBER
node.types.number = 3

# SIMULATION
simulation.log.detailedscaling = false
simulation.stoptime = -1

# OPERATOR
# max parallelism
operator.max.parallelism = 5
# case considered while computing response time or utilization
# types:
#   - avg
#   - worst
operator.values.computing.case = worst
# Load balancer types:
#   - rr (Round Robin)
operator.load.balancer.type = rr

# APPLICATION
# - single-operator
# - forkjoin
# - debs2019
# - simple-tandem
# - simple-tree
dsp.app.type = single-operator
# SLO
dsp.slo.latency = 0.065
# Method for computing per-operator SLO
# - default
# - heuristic
dsp.slo.operator.method = default

# OPERATOR MANAGER
# types:
#   - do-nothing
#   - threshold
#   - q-learning
#   - fa-q-learning (Q-Learning with function approximation)
#   - deep-q-learning
#   - deep-v-learning
#   - vi (value iteration)
#   - vi-splitq (value iteration, split Q table)
#   - fa-tb-vi (trajectory based value iteration using linear function approximation)
#   - deep-tb-vi (trajectory based value iteration using deep reinforcement learning)
#   - fa-hybrid (fa-tb-vi om for off-line training and fa-q-learning for on-line training)
#   - deep-hybrid (deep-tb-vi om for off-line training and deep-v-learning for on-line training)
edf.om.type = q-learning

# THRESHOLD BASED OM PARAMS
edf.om.threshold = 0.75

# REWARD BASED OM PARAMS
edf.rl.om.reconfiguration.weight = 0.4
edf.rl.om.slo.weight = 0.4
edf.rl.om.resources.weight = 0.2
# max input rate considered to discretize lambda
edf.rl.om.max.input.rate = 600
# lambda levels
edf.rl.om.input.rate.levels = 20
# state representation
# types:
#   - k_lambda
#   - reduced_k_lambda
#   _ general_resources
edf.rl.om.state.representation = k_lambda

# DYNAMIC PROGRAMMING
edf.dp.gamma = 0.99

# VI OM PARAMS
edf.vi.max.iterations = 0
edf.vi.max.time.seconds = 60
edf.vi.theta = 1E-4

# TBVI OM PARAMS
edf.tbvi.exec.iterations = 300000
edf.tbvi.exec.seconds = 0
edf.tbvi.trajectory.length = 512
# TBVI with function approximation
edf.tbvi.fa.alpha = 0.1

# Q-LEARNING OM PARAMS
edf.ql.om.alpha = 1.0
edf.ql.om.alpha.decay = 0.98
edf.ql.om.alpha.min.value = 0.1
edf.ql.om.alpha.decay.steps = 10

# DEEP-LEARNING OM PARAMS
edf.dl.om.nd4j.random.seed = 1234L
edf.dl.om.enable.network.ui = false
edf.dl.samples.memory.size = 10000
edf.dl.samples.memory.batch = 32

# ACTION SELECTION POLICY
# types:
#   - random
#   - greedy
#   - e-greedy (epsilon greedy)
asp.type = e-greedy

# RANDOM ASP PARAMS
asp.r.random.seed = 1234L

# E-GREEDY ASP PARAMS
asp.eg.epsilon = 1.0
asp.eg.epsilon.decay = 0.95
asp.eg.epsilon.min.value = 0.01
asp.eg.om.epsilon.decay.steps = 1
asp.eg.epsilon.random.seed = 1234L

# APPLICATION MANAGER
#  - do-nothing
#  - splitq-based
#  - centralized
edf.am.type = do-nothing
